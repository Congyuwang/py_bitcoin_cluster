# 地址聚类及导出

\* 注：以下所有代码，不一定能直接运行，有做轻微简化以清晰展现处理逻辑，除了UnionFind代码是完整的。

## UnionFind

```python
import numpy as np
from numba import njit

class WeightedQuickUnion(object):
    def __init__(self, n):
        self.id = np.arange(n, dtype=np.int32)
        self.sz = np.ones((n,), dtype=np.int32)
    def find(self, p):
        return find_jit(self.id, p)
    def union(self, p, q):
        union_jit(self.id, self.sz, p, q)
    def roots(self):
        return roots_jit(self.id)

@njit
def find_jit(ids, p):
    j = p
    while j != ids[j]:
        ids[j] = ids[ids[j]]
        j = ids[j]
    return j

@njit
def union_jit(ids, sz, p, q):
    idp = find_jit(ids, p)
    idq = find_jit(ids, q)
    if idp != idq:
        if sz[idp] < sz[idq]:
            ids[idp] = idq
            sz[idq] += sz[idp]
        else:
            ids[idq] = idp
            sz[idp] += sz[idq]

@njit
def roots_jit(ids):
    count = len(ids)
    roots = np.zeros(count, dtype=np.int32)
    for k in range(count):
        roots[k] = find_jit(ids, k)
    return roots
```

参考代码[《算法4》1.5 - Union-Find 算法，Python实现](https://www.jianshu.com/p/72da76a34db1)以及[union_find.py](https://gist.github.com/artkpv/6f0591c01a940d6ebe1344a8efa88847)。

## Clustering算法

### Step 0: 初始化

```python
from rocksdict import Rdict
from bitcoin_explorer import BitcoinDB
# 省略其他import

# 新建bitcoin core读取器
db = BitcoinDB(PATH_TO_BITCOIN_CORE)
block_count = db.get_block_count()

# current_index记录地址编号
current_index = 0

# address_to_index：储存在硬盘上的地址到编号的mapping
address_to_index = Rdict(ADDRESS_TO_INDEX)
```

### Step 1: 地址编号

```python
def add_new_address(key: str):
    """Add a new address."""
    # 如果没出现过的话
    if key not in address_to_index:
        # 写入地址 -> 编号
        address_to_index[key] = current_index
        # 编号+1
        current_index += 1
```

因为新地址只会首次出现在output中而不是input中，这一步可以直接忽略inputs。

目前的编号逻辑：

```python
# loop block
for block in db.get_block_iter_range(block_count):
  	# loop transaction
    for trans in block['txdata']:
        # loop output
        for o in trans['output']:
            if o['addresses']:
                add_new_address(o['addresses'][0])
```

**注意这里目前没有处理Multisignature**。建议将改为：

```python
# loop block
for block in db.get_block_iter_range(block_count):
  	# loop transaction
    for trans in block['txdata']:
        # loop output
        for o in trans['output']:
          	# loop address (multisig perhaps)
          	for addr in o['addresses']:
                add_new_address(addr)
```

### Step 2: 地址聚类

cluster逻辑：就是所有input addresses一定属于同一个人

```python
qf = WeightedQuickUnion(current_index)
# loop block
for block in db.get_block_iter_range(block_count, connected=True):
  	# loop transaction
    for trans in block['txdata']:
        # 获取input地址的index
        address_index = [address_to_index[inp['addresses'][0]]
                      	 for inp in trans['input'] if inp['addresses']]
				# union这些地址
        for p, q in zip(left_index[1:], left_index[:-1]):
            qf.union(p, q)
            
# 保存cluster结果
np.save(CLUSTER_FILE, qf.roots())
```

**注意这里目前没有处理Multisignature：**可以将address_index改为：

```python
# 将所有input的addresses展平
address_index = [address_to_index[addr]
                 for inp in trans['input']
                 for addr in inp['addresses']]
```

### Step3: 导出数据为parquet形式

写parquet的函数，tx_num参数为文件编号

```python
def write_parquet(input_data: dict, output_data: dict, tx_num):
    """write to parquet."""
    pd.DataFrame(input_data).to_parquet(INPUT_FOLDER
                                        / (FILE_NAME.format(name=tx_num,
                                                            ext="input")))
    pd.DataFrame(output_data).to_parquet(OUTPUT_FOLDER
                                         / (FILE_NAME.format(name=tx_num,
                                                             ext="output")))
```

新建一张表、以及往表里新加一行数据的函数，注意`address_id = address_map[addresses[0]]`这一行有可能忽略multisig的情况

```python
def new_dat() -> dict:
    """create a new table."""
    return {"time": [],
            "amount": [],
            "address_cluster": [],
            "address": [],
            "transaction": [],
            "block": []}


def append(dat,time, txid, block_height, new_data,
           address_map: Rdict, clusters):
    """append new data to table"""
    addresses = new_data["addresses"]
    if addresses:
        # address to integer
        address_id = address_map[addresses[0]]
        # write data to columns
        dat["time"].append(time)
        dat["amount"].append(new_data["value"])
        dat["address_cluster"].append(clusters[address_id])
        dat["address"].append(address_id)
        dat["transaction"].append(txid)
        dat["block"].append(block_height)
```

导出数据主程序

```python
# 初始化BitcoinDB
db = bit.BitcoinDB(PATH_TO_BITCOIN_CORE)
# 打开地址编号
address_to_index = Rdict(ADDRESS_TO_INDEX)
# 读取clustering结果
cluster = np.load(CLUSTER_FILE)

# 初始化变量

# 当前交易编号
current_transaction = 0
# 当前区块编号
current_block = 0
# 初始化input, output表格
input_dat = new_dat()
output_dat = new_dat()

# loop block
for block in db.get_block_iter_range(db.get_block_count(), 
                                     connected=True):
  	# 区块时间
    block_time = block["header"]["time"]
    # loop transaction
    for trans in block["txdata"]:
        # append output
        for o in trans["output"]:
            append(dat=output_dat,
                   time=block_time,
                   txid=current_transaction,
                   block_height=current_block,
                   new_data=o,
                   address_map=address,
                   clusters=cluster)
        # append input
        for i in trans["input"]:
            append(dat=input_dat,
                   time=block_time,
                   txid=current_transaction,
                   block_height=current_block,
                   new_data=i,
                   address_map=address,
                   clusters=cluster)

        # increment current_transaction
        current_transaction += 1

        # 写满了这张表，分表
        if current_transaction % CHUNK_SIZE == 0:
            # 写入硬盘
            write_parquet(input_data=input_dat,
                          output_data=output_dat,
                          tx_num=current_transaction)
            # 清空表格
            input_dat = new_dat()
            output_dat = new_dat()

    # increment current_block
    current_block += 1

# 写入最后不完整的尾巴
write_parquet(input_data=input_dat,
              output_data=output_dat,
              tx_num=current_transaction)

```